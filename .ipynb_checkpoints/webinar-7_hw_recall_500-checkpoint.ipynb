{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce98df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "# Для работы с матрицами\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "\n",
    "# Матричная факторизация\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight, tfidf_weight\n",
    "\n",
    "\n",
    "# Функции из 1-ого вебинара\n",
    "import os, sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from best_rec_lib.metrics import precision_at_k, ap_k, recall_at_k\n",
    "from best_rec_lib.utils import prefilter_items\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7131624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>basket_id</th>\n",
       "      <th>day</th>\n",
       "      <th>item_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>sales_value</th>\n",
       "      <th>store_id</th>\n",
       "      <th>retail_disc</th>\n",
       "      <th>trans_time</th>\n",
       "      <th>week_no</th>\n",
       "      <th>coupon_disc</th>\n",
       "      <th>coupon_match_disc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851472</td>\n",
       "      <td>1</td>\n",
       "      <td>1004906</td>\n",
       "      <td>1</td>\n",
       "      <td>1.39</td>\n",
       "      <td>364</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1631</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851472</td>\n",
       "      <td>1</td>\n",
       "      <td>1033142</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1631</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id    basket_id  day  item_id  quantity  sales_value  store_id  \\\n",
       "0     2375  26984851472    1  1004906         1         1.39       364   \n",
       "1     2375  26984851472    1  1033142         1         0.82       364   \n",
       "\n",
       "   retail_disc  trans_time  week_no  coupon_disc  coupon_match_disc  \n",
       "0         -0.6        1631        1          0.0                0.0  \n",
       "1          0.0        1631        1          0.0                0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../retail_train.csv')\n",
    "item_features = pd.read_csv('../product.csv')\n",
    "user_features = pd.read_csv('../hh_demographic.csv')\n",
    "\n",
    "# column processing\n",
    "item_features.columns = [col.lower() for col in item_features.columns]\n",
    "user_features.columns = [col.lower() for col in user_features.columns]\n",
    "\n",
    "item_features.rename(columns={'product_id': 'item_id'}, inplace=True)\n",
    "user_features.rename(columns={'household_key': 'user_id'}, inplace=True)\n",
    "\n",
    "# train test split\n",
    "test_size_weeks = 3\n",
    "\n",
    "data_train = data[data['week_no'] < data['week_no'].max() - test_size_weeks]\n",
    "data_test = data[data['week_no'] >= data['week_no'].max() - test_size_weeks]\n",
    "\n",
    "data_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f1c1d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreased # items from 86865 to 5001\n"
     ]
    }
   ],
   "source": [
    "n_items_before = data_train['item_id'].nunique()\n",
    "\n",
    "data_train = prefilter_items(data_train, 5000, item_features)\n",
    "\n",
    "n_items_after = data_train['item_id'].nunique()\n",
    "print('Decreased # items from {} to {}'.format(n_items_before, n_items_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ee40b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>item_id</th>\n",
       "      <th>117847</th>\n",
       "      <th>279994</th>\n",
       "      <th>818981</th>\n",
       "      <th>819255</th>\n",
       "      <th>819308</th>\n",
       "      <th>819400</th>\n",
       "      <th>819487</th>\n",
       "      <th>819590</th>\n",
       "      <th>819594</th>\n",
       "      <th>819840</th>\n",
       "      <th>...</th>\n",
       "      <th>15926775</th>\n",
       "      <th>15926844</th>\n",
       "      <th>15926886</th>\n",
       "      <th>15972074</th>\n",
       "      <th>15972298</th>\n",
       "      <th>15972565</th>\n",
       "      <th>15972790</th>\n",
       "      <th>16100266</th>\n",
       "      <th>16729299</th>\n",
       "      <th>16729415</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 5001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "item_id  117847    279994    818981    819255    819308    819400    819487    \\\n",
       "user_id                                                                         \n",
       "1             0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2             0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "item_id  819590    819594    819840    ...  15926775  15926844  15926886  \\\n",
       "user_id                                ...                                 \n",
       "1             0.0       0.0       0.0  ...       0.0       1.0       0.0   \n",
       "2             0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "\n",
       "item_id  15972074  15972298  15972565  15972790  16100266  16729299  16729415  \n",
       "user_id                                                                        \n",
       "1             0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "2             0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[2 rows x 5001 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_matrix = pd.pivot_table(data_train, \n",
    "                                  index='user_id', columns='item_id', \n",
    "                                  values='quantity', # Можно пробоват ьдругие варианты\n",
    "                                  aggfunc='count', \n",
    "                                  fill_value=0\n",
    "                                 )\n",
    "\n",
    "user_item_matrix = user_item_matrix.astype(float) # необходимый тип матрицы для implicit\n",
    "\n",
    "# переведем в формат saprse matrix\n",
    "sparse_user_item = csr_matrix(user_item_matrix).tocsr()\n",
    "\n",
    "user_item_matrix.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b2fbf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test[data_test['item_id'].isin(data_train['item_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2497d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[856942, 865456, 951954, 971585, 979707, 99065...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[920626]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                             actual\n",
       "0        1  [856942, 865456, 951954, 971585, 979707, 99065...\n",
       "1        3                                           [920626]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = data_test.groupby('user_id')['item_id'].unique().reset_index()\n",
    "result.columns=['user_id', 'actual']\n",
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3841523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "userids = user_item_matrix.index.values\n",
    "itemids = user_item_matrix.columns.values\n",
    "\n",
    "matrix_userids = np.arange(len(userids))\n",
    "matrix_itemids = np.arange(len(itemids))\n",
    "\n",
    "id_to_itemid = dict(zip(matrix_itemids, itemids))\n",
    "id_to_userid = dict(zip(matrix_userids, userids))\n",
    "\n",
    "itemid_to_id = dict(zip(itemids, matrix_itemids))\n",
    "userid_to_id = dict(zip(userids, matrix_userids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648dd734",
   "metadata": {},
   "source": [
    "##### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97f8247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = (SparkSession.builder.config('spark.executor.memory', \"1500mb\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"localhost\")\n",
    "    .config('spark.executor.instances', 4)\n",
    "    .config('spark.executor.cores', 4)\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .master(\"local[*]\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c4857d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b70063f130>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc1d28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3d3bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_spark = data_train[[\"user_id\", \"item_id\", \"quantity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda59a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_data_train = session.createDataFrame(df_to_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc508fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_data_train = spark_data_train.withColumnRenamed(\"quantity\", \"relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bd8bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_spark(df, N=5):\n",
    "    spam = list(df.item_id)\n",
    "    if 999999 in spam:\n",
    "        spam.remove(999999)\n",
    "    \n",
    "    return spam[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dcf3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_spark(rec_met, result, spark_data_train, factors, reg_st, iterations, alpha, N=5):\n",
    "    i = 1\n",
    "    for fact in factors:\n",
    "        for reg in reg_st:\n",
    "            for itrn in iterations:\n",
    "                for alp in alpha:\n",
    "                    \n",
    "                    print(f'{i}/{len(factors)*len(reg_st)*len(iterations)*len(alpha)}', fact, reg, itrn, alp)\n",
    "\n",
    "                    model = ALS(rank=fact, \n",
    "                                userCol=\"user_id\",\n",
    "                                itemCol=\"item_id\", \n",
    "                                ratingCol=\"relevance\", \n",
    "                                implicitPrefs=True, # тип ALS\n",
    "                                coldStartStrategy=\"drop\",\n",
    "                                maxIter=itrn, \n",
    "                                alpha=alp, \n",
    "                                regParam=reg,\n",
    "                                seed=42\n",
    "                    ).fit(spark_data_train)\n",
    "\n",
    "                    # предсказания для всех пользователей\n",
    "                    recs_als = model.recommendForAllUsers(6)\n",
    "\n",
    "                    # Разворачиваем рекомендации через функцию explode\n",
    "                    recs_als = (recs_als\n",
    "                                .withColumn(\"recommendations\", sf.explode(\"recommendations\"))\n",
    "                                .withColumn(\"item_id\", sf.col(\"recommendations.item_id\"))\n",
    "                                .withColumn(\"relevance\", sf.col(\"recommendations.rating\").cast(DoubleType()),)\n",
    "                                .select(\"user_id\", \"item_id\", \"relevance\")\n",
    "                        )\n",
    "\n",
    "                    recs_als = recs_als.toPandas()\n",
    "                    users = list(set(recs_als.user_id))\n",
    "                    new_test_users = set(result['user_id']) - set(users)\n",
    "                    \n",
    "                    if new_test_users:\n",
    "                        result = result[~result['user_id'].isin(new_test_users)]\n",
    "\n",
    "\n",
    "                    result[f'als_spark_{fact}_{reg}_{itrn}'] = result['user_id'].map(lambda x: get_recommendations_spark(recs_als.loc[recs_als['user_id'] == x], N))\n",
    "                    rec_met[f'als_spark_{fact}_{reg}_{itrn}'] = result.apply(lambda row: recall_at_k(row[f'als_spark_{fact}_{reg}_{itrn}'], row['actual'], N), axis=1).mean()\n",
    "                    \n",
    "                    i += 1\n",
    "                    \n",
    "    return result, rec_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff358ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Словарь метрик\n",
    "rec_met = dict()\n",
    "\n",
    "# Списки гиперпараметров для моделей\n",
    "factors = [200, 250, 300, 350, 500]\n",
    "reg_st = [0.03, 0.05, 0.08]\n",
    "alpha = [0.05, 0.1, 0.25, 0.5]\n",
    "iterations = [5, 8, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73cc089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/300 200 0.03 5 0.05\n",
      "2/300 200 0.03 5 0.1\n",
      "3/300 200 0.03 5 0.25\n",
      "4/300 200 0.03 5 0.5\n",
      "5/300 200 0.03 8 0.05\n",
      "6/300 200 0.03 8 0.1\n",
      "7/300 200 0.03 8 0.25\n",
      "8/300 200 0.03 8 0.5\n",
      "9/300 200 0.03 10 0.05\n",
      "10/300 200 0.03 10 0.1\n",
      "11/300 200 0.03 10 0.25\n",
      "12/300 200 0.03 10 0.5\n",
      "13/300 200 0.03 15 0.05\n",
      "14/300 200 0.03 15 0.1\n",
      "15/300 200 0.03 15 0.25\n",
      "16/300 200 0.03 15 0.5\n",
      "17/300 200 0.03 20 0.05\n",
      "18/300 200 0.03 20 0.1\n",
      "19/300 200 0.03 20 0.25\n",
      "20/300 200 0.03 20 0.5\n",
      "21/300 200 0.05 5 0.05\n",
      "22/300 200 0.05 5 0.1\n",
      "23/300 200 0.05 5 0.25\n",
      "24/300 200 0.05 5 0.5\n",
      "25/300 200 0.05 8 0.05\n",
      "26/300 200 0.05 8 0.1\n",
      "27/300 200 0.05 8 0.25\n",
      "28/300 200 0.05 8 0.5\n",
      "29/300 200 0.05 10 0.05\n",
      "30/300 200 0.05 10 0.1\n",
      "31/300 200 0.05 10 0.25\n",
      "32/300 200 0.05 10 0.5\n",
      "33/300 200 0.05 15 0.05\n",
      "34/300 200 0.05 15 0.1\n",
      "35/300 200 0.05 15 0.25\n",
      "36/300 200 0.05 15 0.5\n",
      "37/300 200 0.05 20 0.05\n",
      "38/300 200 0.05 20 0.1\n",
      "39/300 200 0.05 20 0.25\n",
      "40/300 200 0.05 20 0.5\n",
      "41/300 200 0.08 5 0.05\n",
      "42/300 200 0.08 5 0.1\n",
      "43/300 200 0.08 5 0.25\n",
      "44/300 200 0.08 5 0.5\n",
      "45/300 200 0.08 8 0.05\n",
      "46/300 200 0.08 8 0.1\n",
      "47/300 200 0.08 8 0.25\n",
      "48/300 200 0.08 8 0.5\n",
      "49/300 200 0.08 10 0.05\n",
      "50/300 200 0.08 10 0.1\n",
      "51/300 200 0.08 10 0.25\n",
      "52/300 200 0.08 10 0.5\n",
      "53/300 200 0.08 15 0.05\n",
      "54/300 200 0.08 15 0.1\n",
      "55/300 200 0.08 15 0.25\n",
      "56/300 200 0.08 15 0.5\n",
      "57/300 200 0.08 20 0.05\n",
      "58/300 200 0.08 20 0.1\n",
      "59/300 200 0.08 20 0.25\n",
      "60/300 200 0.08 20 0.5\n",
      "61/300 250 0.03 5 0.05\n",
      "62/300 250 0.03 5 0.1\n",
      "63/300 250 0.03 5 0.25\n",
      "64/300 250 0.03 5 0.5\n",
      "65/300 250 0.03 8 0.05\n",
      "66/300 250 0.03 8 0.1\n",
      "67/300 250 0.03 8 0.25\n",
      "68/300 250 0.03 8 0.5\n",
      "69/300 250 0.03 10 0.05\n",
      "70/300 250 0.03 10 0.1\n",
      "71/300 250 0.03 10 0.25\n",
      "72/300 250 0.03 10 0.5\n",
      "73/300 250 0.03 15 0.05\n",
      "74/300 250 0.03 15 0.1\n",
      "75/300 250 0.03 15 0.25\n",
      "76/300 250 0.03 15 0.5\n",
      "77/300 250 0.03 20 0.05\n",
      "78/300 250 0.03 20 0.1\n",
      "79/300 250 0.03 20 0.25\n",
      "80/300 250 0.03 20 0.5\n",
      "81/300 250 0.05 5 0.05\n",
      "82/300 250 0.05 5 0.1\n",
      "83/300 250 0.05 5 0.25\n",
      "84/300 250 0.05 5 0.5\n",
      "85/300 250 0.05 8 0.05\n",
      "86/300 250 0.05 8 0.1\n",
      "87/300 250 0.05 8 0.25\n",
      "88/300 250 0.05 8 0.5\n",
      "89/300 250 0.05 10 0.05\n",
      "90/300 250 0.05 10 0.1\n",
      "91/300 250 0.05 10 0.25\n",
      "92/300 250 0.05 10 0.5\n",
      "93/300 250 0.05 15 0.05\n",
      "94/300 250 0.05 15 0.1\n",
      "95/300 250 0.05 15 0.25\n",
      "96/300 250 0.05 15 0.5\n",
      "97/300 250 0.05 20 0.05\n",
      "98/300 250 0.05 20 0.1\n",
      "99/300 250 0.05 20 0.25\n",
      "100/300 250 0.05 20 0.5\n",
      "101/300 250 0.08 5 0.05\n",
      "102/300 250 0.08 5 0.1\n",
      "103/300 250 0.08 5 0.25\n",
      "104/300 250 0.08 5 0.5\n",
      "105/300 250 0.08 8 0.05\n",
      "106/300 250 0.08 8 0.1\n",
      "107/300 250 0.08 8 0.25\n",
      "108/300 250 0.08 8 0.5\n",
      "109/300 250 0.08 10 0.05\n",
      "110/300 250 0.08 10 0.1\n",
      "111/300 250 0.08 10 0.25\n",
      "112/300 250 0.08 10 0.5\n",
      "113/300 250 0.08 15 0.05\n",
      "114/300 250 0.08 15 0.1\n",
      "115/300 250 0.08 15 0.25\n",
      "116/300 250 0.08 15 0.5\n",
      "117/300 250 0.08 20 0.05\n",
      "118/300 250 0.08 20 0.1\n",
      "119/300 250 0.08 20 0.25\n",
      "120/300 250 0.08 20 0.5\n",
      "121/300 300 0.03 5 0.05\n",
      "122/300 300 0.03 5 0.1\n",
      "123/300 300 0.03 5 0.25\n",
      "124/300 300 0.03 5 0.5\n",
      "125/300 300 0.03 8 0.05\n",
      "126/300 300 0.03 8 0.1\n",
      "127/300 300 0.03 8 0.25\n",
      "128/300 300 0.03 8 0.5\n",
      "129/300 300 0.03 10 0.05\n",
      "130/300 300 0.03 10 0.1\n",
      "131/300 300 0.03 10 0.25\n",
      "132/300 300 0.03 10 0.5\n",
      "133/300 300 0.03 15 0.05\n",
      "134/300 300 0.03 15 0.1\n",
      "135/300 300 0.03 15 0.25\n",
      "136/300 300 0.03 15 0.5\n",
      "137/300 300 0.03 20 0.05\n",
      "138/300 300 0.03 20 0.1\n",
      "139/300 300 0.03 20 0.25\n",
      "140/300 300 0.03 20 0.5\n",
      "141/300 300 0.05 5 0.05\n",
      "142/300 300 0.05 5 0.1\n",
      "143/300 300 0.05 5 0.25\n",
      "144/300 300 0.05 5 0.5\n",
      "145/300 300 0.05 8 0.05\n",
      "146/300 300 0.05 8 0.1\n",
      "147/300 300 0.05 8 0.25\n",
      "148/300 300 0.05 8 0.5\n",
      "149/300 300 0.05 10 0.05\n",
      "150/300 300 0.05 10 0.1\n",
      "151/300 300 0.05 10 0.25\n",
      "152/300 300 0.05 10 0.5\n",
      "153/300 300 0.05 15 0.05\n",
      "154/300 300 0.05 15 0.1\n",
      "155/300 300 0.05 15 0.25\n",
      "156/300 300 0.05 15 0.5\n",
      "157/300 300 0.05 20 0.05\n",
      "158/300 300 0.05 20 0.1\n",
      "159/300 300 0.05 20 0.25\n",
      "160/300 300 0.05 20 0.5\n",
      "161/300 300 0.08 5 0.05\n",
      "162/300 300 0.08 5 0.1\n",
      "163/300 300 0.08 5 0.25\n",
      "164/300 300 0.08 5 0.5\n",
      "165/300 300 0.08 8 0.05\n",
      "166/300 300 0.08 8 0.1\n",
      "167/300 300 0.08 8 0.25\n",
      "168/300 300 0.08 8 0.5\n",
      "169/300 300 0.08 10 0.05\n",
      "170/300 300 0.08 10 0.1\n",
      "171/300 300 0.08 10 0.25\n",
      "172/300 300 0.08 10 0.5\n",
      "173/300 300 0.08 15 0.05\n",
      "174/300 300 0.08 15 0.1\n",
      "175/300 300 0.08 15 0.25\n",
      "176/300 300 0.08 15 0.5\n",
      "177/300 300 0.08 20 0.05\n",
      "178/300 300 0.08 20 0.1\n",
      "179/300 300 0.08 20 0.25\n",
      "180/300 300 0.08 20 0.5\n",
      "181/300 350 0.03 5 0.05\n",
      "182/300 350 0.03 5 0.1\n",
      "183/300 350 0.03 5 0.25\n",
      "184/300 350 0.03 5 0.5\n",
      "185/300 350 0.03 8 0.05\n",
      "186/300 350 0.03 8 0.1\n",
      "187/300 350 0.03 8 0.25\n",
      "188/300 350 0.03 8 0.5\n",
      "189/300 350 0.03 10 0.05\n",
      "190/300 350 0.03 10 0.1\n",
      "191/300 350 0.03 10 0.25\n",
      "192/300 350 0.03 10 0.5\n",
      "193/300 350 0.03 15 0.05\n",
      "194/300 350 0.03 15 0.1\n",
      "195/300 350 0.03 15 0.25\n",
      "196/300 350 0.03 15 0.5\n",
      "197/300 350 0.03 20 0.05\n",
      "198/300 350 0.03 20 0.1\n",
      "199/300 350 0.03 20 0.25\n",
      "200/300 350 0.03 20 0.5\n",
      "201/300 350 0.05 5 0.05\n",
      "202/300 350 0.05 5 0.1\n",
      "203/300 350 0.05 5 0.25\n",
      "204/300 350 0.05 5 0.5\n",
      "205/300 350 0.05 8 0.05\n",
      "206/300 350 0.05 8 0.1\n",
      "207/300 350 0.05 8 0.25\n",
      "208/300 350 0.05 8 0.5\n",
      "209/300 350 0.05 10 0.05\n",
      "210/300 350 0.05 10 0.1\n",
      "211/300 350 0.05 10 0.25\n",
      "212/300 350 0.05 10 0.5\n",
      "213/300 350 0.05 15 0.05\n",
      "214/300 350 0.05 15 0.1\n",
      "215/300 350 0.05 15 0.25\n",
      "216/300 350 0.05 15 0.5\n",
      "217/300 350 0.05 20 0.05\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o24462.fit.\n: org.apache.spark.SparkException: Job 6497 cancelled because SparkContext was shut down\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1188)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1186)\r\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\r\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1186)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2887)\r\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\r\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2784)\r\n\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2095)\r\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\r\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2095)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\r\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\r\n\tat org.apache.spark.ml.recommendation.ALS$.computeYtY(ALS.scala:1777)\r\n\tat org.apache.spark.ml.recommendation.ALS$.computeFactors(ALS.scala:1699)\r\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$train$8(ALS.scala:1015)\r\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$train$8$adapted(ALS.scala:1011)\r\n\tat scala.collection.immutable.Range.foreach(Range.scala:158)\r\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:1011)\r\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:722)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:704)\r\n\tat sun.reflect.GeneratedMethodAccessor210.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mals_spark\u001b[1;34m(rec_met, result, spark_data_train, factors, reg_st, iterations, alpha, N)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alp \u001b[38;5;129;01min\u001b[39;00m alpha:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(factors)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(reg_st)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(iterations)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(alpha)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, fact, reg, itrn, alp)\n\u001b[1;32m---> 10\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mALS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43muserCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mitemCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitem_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mratingCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelevance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimplicitPrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# тип ALS\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcoldStartStrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmaxIter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitrn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mregParam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_data_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# предсказания для всех пользователей\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     recs_als \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrecommendForAllUsers(\u001b[38;5;241m6\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py:379\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 379\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py:376\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o24462.fit.\n: org.apache.spark.SparkException: Job 6497 cancelled because SparkContext was shut down\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1188)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1186)\r\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\r\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1186)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2887)\r\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\r\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2784)\r\n\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2095)\r\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\r\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2095)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\r\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\r\n\tat org.apache.spark.ml.recommendation.ALS$.computeYtY(ALS.scala:1777)\r\n\tat org.apache.spark.ml.recommendation.ALS$.computeFactors(ALS.scala:1699)\r\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$train$8(ALS.scala:1015)\r\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$train$8$adapted(ALS.scala:1011)\r\n\tat scala.collection.immutable.Range.foreach(Range.scala:158)\r\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:1011)\r\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:722)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:704)\r\n\tat sun.reflect.GeneratedMethodAccessor210.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result, rec_met = als_spark(rec_met, result, spark_data_train, factors, reg_st, iterations, alpha, N=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085981e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9f20bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [rec_met]\n",
    "data = pd.DataFrame(rec_met, index =['Recall@k'])\n",
    "data = data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8de29706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recall@k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>als_spark_350_0.03_5</th>\n",
       "      <td>0.103738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_350_0.03_10</th>\n",
       "      <td>0.103149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_350_0.03_8</th>\n",
       "      <td>0.103097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_350_0.05_8</th>\n",
       "      <td>0.103009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_350_0.05_10</th>\n",
       "      <td>0.102947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_350_0.05_15</th>\n",
       "      <td>0.102747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.03_20</th>\n",
       "      <td>0.102709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_350_0.03_15</th>\n",
       "      <td>0.102623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.03_5</th>\n",
       "      <td>0.102584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_350_0.03_20</th>\n",
       "      <td>0.102512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.03_8</th>\n",
       "      <td>0.102502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.03_15</th>\n",
       "      <td>0.102466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.03_10</th>\n",
       "      <td>0.102189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.05_20</th>\n",
       "      <td>0.101859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.05_15</th>\n",
       "      <td>0.101788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.03_5</th>\n",
       "      <td>0.101612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.03_8</th>\n",
       "      <td>0.101437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.03_20</th>\n",
       "      <td>0.101434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_350_0.05_5</th>\n",
       "      <td>0.101329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.03_10</th>\n",
       "      <td>0.101200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.05_8</th>\n",
       "      <td>0.101103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.05_10</th>\n",
       "      <td>0.100897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.03_15</th>\n",
       "      <td>0.100562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.05_5</th>\n",
       "      <td>0.100467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.05_10</th>\n",
       "      <td>0.099775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.05_8</th>\n",
       "      <td>0.099769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.05_15</th>\n",
       "      <td>0.099738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.03_15</th>\n",
       "      <td>0.099484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.05_20</th>\n",
       "      <td>0.099349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.03_10</th>\n",
       "      <td>0.099064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.03_20</th>\n",
       "      <td>0.098782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.03_8</th>\n",
       "      <td>0.098050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.05_5</th>\n",
       "      <td>0.097792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.03_5</th>\n",
       "      <td>0.097686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.05_20</th>\n",
       "      <td>0.095829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.05_15</th>\n",
       "      <td>0.095286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.05_8</th>\n",
       "      <td>0.095008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.05_10</th>\n",
       "      <td>0.094936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.08_15</th>\n",
       "      <td>0.093796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.08_10</th>\n",
       "      <td>0.093729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.08_20</th>\n",
       "      <td>0.093633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.05_5</th>\n",
       "      <td>0.093506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.08_5</th>\n",
       "      <td>0.093278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_300_0.08_8</th>\n",
       "      <td>0.092817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.08_20</th>\n",
       "      <td>0.090181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.08_8</th>\n",
       "      <td>0.090177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.08_10</th>\n",
       "      <td>0.090108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.08_15</th>\n",
       "      <td>0.090094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_250_0.08_5</th>\n",
       "      <td>0.089837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.08_15</th>\n",
       "      <td>0.088443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.08_10</th>\n",
       "      <td>0.088050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.08_20</th>\n",
       "      <td>0.087974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.08_8</th>\n",
       "      <td>0.087883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>als_spark_200_0.08_5</th>\n",
       "      <td>0.087762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Recall@k\n",
       "als_spark_350_0.03_5   0.103738\n",
       "als_spark_350_0.03_10  0.103149\n",
       "als_spark_350_0.03_8   0.103097\n",
       "als_spark_350_0.05_8   0.103009\n",
       "als_spark_350_0.05_10  0.102947\n",
       "als_spark_350_0.05_15  0.102747\n",
       "als_spark_300_0.03_20  0.102709\n",
       "als_spark_350_0.03_15  0.102623\n",
       "als_spark_300_0.03_5   0.102584\n",
       "als_spark_350_0.03_20  0.102512\n",
       "als_spark_300_0.03_8   0.102502\n",
       "als_spark_300_0.03_15  0.102466\n",
       "als_spark_300_0.03_10  0.102189\n",
       "als_spark_300_0.05_20  0.101859\n",
       "als_spark_300_0.05_15  0.101788\n",
       "als_spark_250_0.03_5   0.101612\n",
       "als_spark_250_0.03_8   0.101437\n",
       "als_spark_250_0.03_20  0.101434\n",
       "als_spark_350_0.05_5   0.101329\n",
       "als_spark_250_0.03_10  0.101200\n",
       "als_spark_300_0.05_8   0.101103\n",
       "als_spark_300_0.05_10  0.100897\n",
       "als_spark_250_0.03_15  0.100562\n",
       "als_spark_300_0.05_5   0.100467\n",
       "als_spark_250_0.05_10  0.099775\n",
       "als_spark_250_0.05_8   0.099769\n",
       "als_spark_250_0.05_15  0.099738\n",
       "als_spark_200_0.03_15  0.099484\n",
       "als_spark_250_0.05_20  0.099349\n",
       "als_spark_200_0.03_10  0.099064\n",
       "als_spark_200_0.03_20  0.098782\n",
       "als_spark_200_0.03_8   0.098050\n",
       "als_spark_250_0.05_5   0.097792\n",
       "als_spark_200_0.03_5   0.097686\n",
       "als_spark_200_0.05_20  0.095829\n",
       "als_spark_200_0.05_15  0.095286\n",
       "als_spark_200_0.05_8   0.095008\n",
       "als_spark_200_0.05_10  0.094936\n",
       "als_spark_300_0.08_15  0.093796\n",
       "als_spark_300_0.08_10  0.093729\n",
       "als_spark_300_0.08_20  0.093633\n",
       "als_spark_200_0.05_5   0.093506\n",
       "als_spark_300_0.08_5   0.093278\n",
       "als_spark_300_0.08_8   0.092817\n",
       "als_spark_250_0.08_20  0.090181\n",
       "als_spark_250_0.08_8   0.090177\n",
       "als_spark_250_0.08_10  0.090108\n",
       "als_spark_250_0.08_15  0.090094\n",
       "als_spark_250_0.08_5   0.089837\n",
       "als_spark_200_0.08_15  0.088443\n",
       "als_spark_200_0.08_10  0.088050\n",
       "als_spark_200_0.08_20  0.087974\n",
       "als_spark_200_0.08_8   0.087883\n",
       "als_spark_200_0.08_5   0.087762"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values(\"Recall@k\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75515575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recall@k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>als_spark_350_0.03_5</th>\n",
       "      <td>0.103738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Recall@k\n",
       "als_spark_350_0.03_5  0.103738"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Лучшие параметры по метрике Recall@5\n",
    "data[data['Recall@k'] == data['Recall@k'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "52ee417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
